{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import boto.s3\n",
    "import logging\n",
    "import smtplib\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import datetime\n",
    "import StringIO\n",
    "\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "from avro import datafile, io, schema\n",
    "from datetime import datetime, timedelta, date\n",
    "from boto.s3.key import Key\n",
    "from boto.s3.connection import S3Connection\n",
    "from collections import defaultdict, Counter\n",
    "from netaddr import IPNetwork, IPAddress\n",
    "\n",
    "from agg_config import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SenderModelReader(object):\n",
    "  def __init__(self, aws_conf, bucket_name, key):\n",
    "    self.s3_conn = S3Connection(**aws_conf)\n",
    "    self.bucket = self.s3_conn.get_bucket(bucket_name)\n",
    "    self.key = key\n",
    "  def fetch(self):\n",
    "    key = self.bucket.get_key(self.key)\n",
    "    data_file = StringIO.StringIO()\n",
    "    key.get_file(data_file)\n",
    "    reader = DataFileReader(data_file, DatumReader())\n",
    "    return [(x['version'], x['threshold'], x['models']) for x in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab user feedback from Cousteau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_conn = psycopg2.connect(host=db_conf['host'], user=db_conf['user'], database=db_conf['database'])\n",
    "cursor = db_conn.cursor()\n",
    "cursor.execute(\"select suggestion, context, organization_id, comments from user_response where suggestion_type = 'authenticity';\")\n",
    "data = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fb_auth = {}\n",
    "org_info = {}\n",
    "fb_comments = {}\n",
    "count = 0\n",
    "for d in data:\n",
    "    suggestion = json.loads(d[0])\n",
    "    context = json.loads(d[1])\n",
    "    oid = d[2]\n",
    "    comments = d[3]\n",
    "    for m in suggestion['matches']:\n",
    "        if m['field'] == 'domain':\n",
    "            domain = m['value'][0]\n",
    "        elif m['field'] == 'ip':\n",
    "            ips = m['value']\n",
    "    if suggestion['input_type'] == 'ip_list':\n",
    "        count += 1\n",
    "        fb_auth[domain] = ips\n",
    "        org_info[domain] = oid\n",
    "        fb_comments[domain] = comments\n",
    "\n",
    "print count\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the latest Sender Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smr = SenderModelReader(aws_conf, 'agari-prod-ep-metadata','models/sendermodels-2016-08-22_02:25:57.avro')\n",
    "sm = smr.fetch()\n",
    "sms = sm[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm_auth = defaultdict(lambda: defaultdict(float))\n",
    "for s in sms:\n",
    "    domain = s['domain']\n",
    "    for x in s['cidrs']:\n",
    "        ip = x['cidr']\n",
    "        score = x['score']\n",
    "        sm_auth[domain][ip] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain = 'pacificskymortgage.com'\n",
    "print fb_auth[domain]\n",
    "print sm_auth[domain]\n",
    "print np.mean(sm_auth[domain].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "miss = 0\n",
    "hit = 0\n",
    "for domain in fb_auth:\n",
    "    try:\n",
    "        q = sm_auth[domain]\n",
    "        hit += 1\n",
    "    except:\n",
    "        miss += 1\n",
    "\n",
    "print 'For domains: {} hits, {} misses'.format(hit, miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fb_scores = defaultdict(lambda: defaultdict(list))\n",
    "fb_ips_map = defaultdict(lambda: defaultdict(list))\n",
    "for domain in fb_auth:\n",
    "    fb_ips = fb_auth[domain]\n",
    "    for fb_ip in fb_ips:\n",
    "        ip_scores = []\n",
    "        for ip in sm_auth[domain]:\n",
    "            if IPAddress(ip) in IPNetwork(fb_ip): # only grab SM ips which have feedback for that domain\n",
    "                fb_scores[domain][fb_ip].append(sm_auth[domain][ip])\n",
    "                fb_ips_map[domain][fb_ip].append(ip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual domain-ip scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "individual_ip_scores = []\n",
    "for d in fb_scores:\n",
    "    for c in fb_scores[d]:\n",
    "        individual_ip_scores += fb_scores[d][c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0.0, 1.0, 50)\n",
    "plt.hist(individual_ip_scores, color='b', bins=bins, alpha=0.5)\n",
    "plt.title('Individual domain-IP auth scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIDR-averaged scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averaged_ip_scores = []\n",
    "for d in fb_scores:\n",
    "    for c in fb_scores[d]:\n",
    "        averaged_ip_scores.append(np.mean(fb_scores[d][c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0.0, 1.0, 50)\n",
    "\n",
    "plt.hist(averaged_ip_scores, color='b', bins=bins, alpha=0.5)\n",
    "plt.title('CIDR-averaged auth scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_cutoff = 0.3\n",
    "dom_ip_to_investigate = defaultdict(list)\n",
    "for d in fb_scores:\n",
    "    comments = fb_comments[d]\n",
    "    for c in fb_scores[d]:\n",
    "        if np.mean(fb_scores[d][c]) <= auth_cutoff:\n",
    "            oid = org_info[d]\n",
    "            ips = fb_ips_map[d][c]\n",
    "            stats = (min(fb_scores[d][c]), np.mean(fb_scores[d][c]), max(fb_scores[d][c]), len(fb_scores[d][c]))\n",
    "            dom_ip_to_investigate[oid].append((d, ips) + stats + (comments,))\n",
    "\n",
    "for o in sorted(dom_ip_to_investigate.keys()):\n",
    "    print 'Org {} - {} items'.format(str(o).rjust(2), str(len(dom_ip_to_investigate[o])).rjust(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org = 54\n",
    "for x in dom_ip_to_investigate[org][:]:\n",
    "    print (x[0], x[1])\n",
    "    #print x[1]\n",
    "    print x[2:-1]\n",
    "    print x[-1]\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cidrs_per_domain = [ len(x) for x in fb_scores.itervalues() ]\n",
    "scores_per_cidr = []\n",
    "for d in fb_scores:\n",
    "    for c in fb_scores[d]:\n",
    "        scores_per_cidr.append(len(fb_scores[d][c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to improve for authenticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "Org 1\n",
    "- (u'theflyzikgroup.com', ['209.17.115.114','209.17.115.115','209.17.115.116','209.17.115.39','209.17.115.43','209.17.115.50'])\n",
    "    - These are fixed - PTR neighborhood bug\n",
    "- (u'cloud.sophos.com', [u'208.70.210.247'])\n",
    "    - These are fixed - PTR neighborhood bug\n",
    "\n",
    "Org 54\n",
    "- (u'coloradocyclist.com', [u'198.2.138.130', u'198.2.131.34', u'198.2.182.209', u'198.2.139.204', u'198.2.183.30', u'198.2.129.201', u'198.2.183.196', u'198.2.129.196', u'198.2.129.194', u'198.2.182.130', u'198.2.182.133', u'198.2.129.93', u'198.2.130.85', u'198.2.129.99', u'198.2.183.122', u'198.2.181.74', u'198.2.130.66', u'198.2.131.16', u'198.2.182.142', u'198.2.130.91', u'198.2.131.240', u'198.2.187.223', u'198.2.183.53', u'198.2.190.234', u'198.2.183.112', u'198.2.183.119', u'198.2.139.167', u'198.2.190.16', u'198.2.129.152', u'198.2.139.154', u'198.2.190.207', u'198.2.138.147', u'198.2.129.175', u'198.2.181.223', u'198.2.190.214', u'198.2.190.215', u'198.2.181.28', u'198.2.130.188', u'198.2.138.168', u'198.2.138.164', u'198.2.130.51', u'198.2.183.9', u'198.2.130.9', u'198.2.182.181', u'198.2.183.93', u'198.2.190.44', u'198.2.181.82', u'198.2.130.4', u'198.2.129.225', u'198.2.129.223', u'198.2.138.159', u'198.2.190.173', u'198.2.138.33', u'198.2.131.162', u'198.2.183.209', u'198.2.182.97', u'198.2.138.227', u'198.2.182.99', u'198.2.139.194', u'198.2.182.7', u'198.2.129.134', u'198.2.190.244', u'198.2.131.172', u'198.2.131.173'])\n",
    "    - ** Mean auth of these is 0.27. Jeez, can I just add a heuristic saying that if >= X PTR-neighborhood IPs send for a domain, it's authentic? **\n",
    "\n",
    "Org 56\n",
    "- https://ep.agari.com/messages?interval_days=54&size=200&sort_by=ts&start_date=2016-07-01&unicode_domain=valleycare.com\n",
    "    - (u'valleycare.com', [u'68.232.129.206', u'68.232.135.60'])\n",
    "    - MX & PTR mismatch - looks like a one-off. EP-1822\n",
    "- https://ep.agari.com/receiver-ip/66.231.95.69?interval_days=60\n",
    "    - (u'uhg.com', [u'66.231.95.69'])\n",
    "    - ** This IP sends 10,000s of messages per day (usually) on behalf of uhg.com **\n",
    "- https://ep.agari.com/messages?interval_days=30&ip=184.94.241.96&start_date=2016-07-25&unicode_domain=cerner.com\n",
    "    - (u'cerner.com', [u'184.94.241.96'])\n",
    "    - ** Comments: \"Top two IPs are in the SPF record, with SPF record updates to sender models we will get these.\" - need to check SPF infra file and make sure this is true **\n",
    "    - ** Also, periodicity for the IP as a whole: https://ep.agari.com/receiver-ip/184.94.241.96?interval_days=60 **\n",
    "- https://ep.agari.com/receiver-domain/uhgrecruitmentservices.com?interval_days=30\n",
    "    ** Holy crap, why are the IPs 169.54.226.{68|69|70} not getting picked up in SM? 18k messages each over 30 days **\n",
    "\n",
    "Org 61\n",
    "- https://ep.agari.com/receiver-ip/174.37.239.162?interval_days=60\n",
    "    - (u'pacbell.net', [u'174.37.239.162'])\n",
    "    - ** sends 2 message every month on the 11th (periodicity feature) **\n",
    "- https://ep.agari.com/receiver-ip/208.87.208.8?interval_days=60\n",
    "    - (u'united.com', [u'208.87.208.8'])\n",
    "    - don't see this anymore\n",
    "- https://ep.agari.com/receiver-ip/83.140.23.85?interval_days=60\n",
    "    - (u'starwoodhotels.com', [u'83.140.23.85'])\n",
    "    - only one message in last few months\n",
    "\n",
    "Org 64:\n",
    "- https://ep.agari.com/receiver-ip/69.63.131.201?interval_days=60&message_type=no_auth\n",
    "    - (u'sru.org', [u'69.63.131.201'])\n",
    "    - 6 messages over 2 days in last 60 days - ???\n",
    "- https://ep.agari.com/messages?interval_days=60&ip=161.253.198.11&start_date=2016-06-24&unicode_domain=wakehealth.edu\n",
    "    - (u'wakehealth.edu', [u'161.253.198.11'])\n",
    "    - appears to be mailing list\n",
    "- (u'acr.org', ['103.28.42.112','167.89.4.19','103.28.42.109','203.55.21.17'])\n",
    "    - asked aarmstrong about these - comment is 'IP address that are part of 3rd parties with IP ranges in the Sender Inventory'\n",
    "    - ** \"If we've marked a known range of IP's from a 3rd party good, and another pops up in the same /24, that should be weighted so thst it's not an instant spoof (assuming same sending characteristics as the marked good neighbord IP)\" - Alex **\n",
    "- https://ep.agari.com/receiver-domain/mdanderson.org?interval_days=60\n",
    "    - (u'mdanderson.org', [u'161.253.198.23','38.105.65.115'])\n",
    "    - ** mailing lists **\n",
    "- https://ep.agari.com/receiver-domain/acep.org?interval_days=60&message_type=no_auth&size=100\n",
    "    - (u'acep.org', [u'198.37.147.137','50.31.33.53','198.37.147.117'])\n",
    "    - ** mailing lists in same PTR neighborhood, but low volume **\n",
    "- https://ep.agari.com/messages?interval_days=63&ip=67.216.225.44&start_date=2016-06-21&unicode_domain=nasci.org\n",
    "    - (u'nasci.org', [u'67.216.225.44'])\n",
    "    - 9 messages all one 1 day - looks pretty inauthentic\n",
    "- https://ep.agari.com/messages?interval_days=60&ip=66.35.59.40&start_date=2016-06-24&unicode_domain=ochsner.org\n",
    "    - (u'ochsner.org', [u'66.35.59.40'])\n",
    "    - ** mailing list, single day/message **\n",
    "\n",
    "Org 66:\n",
    "- (u'identitydirect.co.uk', [u'103.245.145.220'])\n",
    "    - Just a single IP, no neighborhood, low sending volume - nothing to see here\n",
    "\n",
    "Org 67:\n",
    "- (u'spencerstuart.com', [u'79.170.244.144'])\n",
    "    - Doesn't show up in prod anymore\n",
    "- https://ep.agari.com/messages?interval_days=60&ip=149.169.2.72&start_date=2016-06-25&unicode_domain=pps.agari\n",
    "    - (u'pps.agari', [u'149.169.2.72'])\n",
    "    - ** Marked as a forwarder (.edu host) -- potential forwarder IP hopping candidate **\n",
    "\n",
    "Org 69:\n",
    "- (u'ip-172-18-19-67.ec2.internal', [u'184.72.111.30']) AND (u'ip-172-18-1-73.ec2.internal', [u'52.90.100.134'])\n",
    "    - ** These are EC2 instances sending alerts. They send messages to a lot of people, but only on a few days - is there potential for a breadth-type feature here? **\n",
    "\n",
    "\n",
    "** - **\n",
    "** Other things to note **\n",
    "- When we see an absolutely new domain for the first time, and it's scored via incremental, it will get a 5.0 identity trust score. However, once we run a full model build, the reputation will be lower than this - usually around 4.3. This is because the full model build uses the consistency feature, while incremental does not. Should we change this? It looks weird for a domain to start at 5.0, drop to 4.something, and then potentially rise up again as we see more traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print sm_auth['bankofamerica.com']['121.33.38.170']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab reputation feedback from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_conn = psycopg2.connect(host=db_conf['host'], user=db_conf['user'], database=db_conf['database'])\n",
    "cursor = db_conn.cursor()\n",
    "cursor.execute(\"select suggestion, context, organization_id, comments from user_response where suggestion_type = 'reputation';\")\n",
    "data2 = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fb_rep = {}\n",
    "for x in data2:\n",
    "    rule = json.loads(x[0])\n",
    "    domain = rule['matches'][0]['value'][0]\n",
    "    org = int(x[2])\n",
    "    if rule['label'] == 'trusted':\n",
    "        fb_rep[domain] = str(org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orgs = Counter(fb_rep.values())\n",
    "for x in sorted(orgs):\n",
    "    print x, orgs[x]\n",
    "print type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab reputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_name = 'agari-prod-ep-metadata'\n",
    "rep_key = 'activereputation/reputation-1472029516.15.json'\n",
    "rep = json.load(S3Connection(**aws_conf).get_bucket(bucket_name).get_key(rep_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_rep = defaultdict(lambda: defaultdict(list))\n",
    "for x in rep:\n",
    "    domain = x[0]\n",
    "    for org in x[1]:\n",
    "        model_rep[domain][str(org)] = x[1][org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rep_scores = defaultdict(lambda: defaultdict(list))\n",
    "for domain, org in fb_rep.iteritems():\n",
    "    rep_scores[domain][str(org)] = model_rep[domain][str(org)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rep_scores_hist = []\n",
    "miss = 0\n",
    "for domain, scores in rep_scores.iteritems():\n",
    "    for org in scores:\n",
    "        try:\n",
    "            rep_scores_hist.append(1 - scores[org][0])\n",
    "        except:\n",
    "            miss += 1\n",
    "\n",
    "print '{} scores missed'.format(miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0.0, 1.0, 50)\n",
    "\n",
    "plt.hist(rep_scores_hist, color='b', bins=bins, alpha=0.5)\n",
    "plt.title('Reputation scores - trusted feedback')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_reasons = []\n",
    "miss = 0\n",
    "for domain, scores in rep_scores.iteritems():\n",
    "    for org in scores:\n",
    "        try:\n",
    "            if (1 - scores[org][0]) < 0.3:\n",
    "                score_reasons.append((domain, org,scores[org]))\n",
    "        except:\n",
    "            miss += 1\n",
    "\n",
    "print '{} scores missed\\n\\n'.format(miss)\n",
    "\n",
    "for s in score_reasons:\n",
    "    print s[0]\n",
    "    print s[1]\n",
    "    print s[2], '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to improve for reputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Whitelist AWS? - \"ec2.internal\" hdr_from domain suffix? Or a domain suffix rule for reputation? e.g. we've seen a lot of XXXX.ec2.internal domains, can we improve reputation for them all? **\n",
    "* ** Removing cousin domain hit if registration age is old enough? **\n",
    "* ** Should .gov domains be considered as pop-up? **\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

